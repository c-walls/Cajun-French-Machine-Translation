{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\miniconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def load_model(base_model, legacy=False):\n",
    "    tokens_to_add = ['<|Eng|>', '<|CajFr|>']\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model, clean_up_tokenization_spaces=True, legacy=legacy, additional_special_tokens=tokens_to_add)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
    "    model.cuda()\n",
    "    return tokenizer, model\n",
    "\n",
    "def data_generator(dataset, tokenizer, batch_size=32):\n",
    "    np.random.shuffle(dataset)\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        raw_batch = dataset[i:i + batch_size]\n",
    "        batch_data = transform_batch(raw_batch, tokenizer)\n",
    "        yield batch_data\n",
    "\n",
    "def transform_batch(batch, tokenizer):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    longest_seq = max([max(len(sentence['English']), len(sentence['Cajun French'])) for sentence in batch])\n",
    "    max_seq_len = min(128, longest_seq + 4)\n",
    "    for sentence_pair in batch:\n",
    "        input_ids, target_ids = format_translation_data(sentence_pair, tokenizer, max_seq_len)\n",
    "        inputs.append(input_ids)\n",
    "        targets.append(target_ids)\n",
    "    batch_input_ids = torch.cat(inputs).cuda()\n",
    "    batch_target_ids = torch.cat(targets).cuda()\n",
    "    return batch_input_ids, batch_target_ids\n",
    "\n",
    "def format_translation_data(data, tokenizer, max_seq_len=128):\n",
    "    input_lang, target_lang = np.random.choice(['English', 'Cajun French'], size=2, replace=False)\n",
    "    input_text = data[input_lang]\n",
    "    target_text = data[target_lang]\n",
    "    input_ids = tokenizer.encode(text=input_text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_seq_len).cuda()\n",
    "    target_ids = tokenizer.encode(text=target_text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_seq_len).cuda()\n",
    "    return input_ids, target_ids\n",
    "\n",
    "def eval_model(model, tokenizer, test_data, batch_size):\n",
    "    model.eval()\n",
    "    eval_generator = data_generator(test_data, tokenizer, batch_size)\n",
    "    with torch.no_grad():\n",
    "        eval_loss = []\n",
    "        for batch in eval_generator:\n",
    "            input_ids, target_ids = batch\n",
    "            loss = model(input_ids, labels=target_ids).loss\n",
    "            eval_loss.append(loss.item())\n",
    "    return np.mean(eval_loss)\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference], hypothesis, smoothing_function=smoothie)\n",
    "\n",
    "with open('Data/corpus.json', 'r', encoding='utf-8') as file:\n",
    "    corpus_data = json.load(file)['data']\n",
    "    testing_data = corpus_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU scores for local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_5552\\2100876610.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('../Models/nllb_1_model.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = 'facebook/nllb-200-1.3B'\n",
    "tokenizer, model = load_model(base_model)\n",
    "model.load_state_dict(torch.load('../Models/nllb_1_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = data_generator(testing_data, tokenizer, 5)\n",
    "\n",
    "bleu_scores = []\n",
    "\n",
    "for input_ids, target_ids in eval_generator:\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        temperature=0.75,\n",
    "        repetition_penalty=1.5,\n",
    "        max_length=128,\n",
    "        num_beams=15,\n",
    "        num_return_sequences=1,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the target and output sequences for comparison\n",
    "    target_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in target_ids]\n",
    "    output_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output]\n",
    "\n",
    "    # Calculate BLEU scores for the batch\n",
    "    for target, output in zip(target_texts, output_texts):\n",
    "        reference = target.split()\n",
    "        hypothesis = output.split()\n",
    "        bleu_score = calculate_bleu(reference, hypothesis)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "# Print average BLEU score\n",
    "average_bleu_score = np.mean(bleu_scores)\n",
    "print(f\"Average BLEU score: {average_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Gemini API to calculate BLEU score on fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import os\n",
    "\n",
    "api_endpoint = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent\"\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Function to call the Google Gemini API\n",
    "def translate_with_gemini(input_text):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"text\": input_text\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(f\"{api_endpoint}?key={api_key}\", headers=headers, json=payload)\n",
    "    response_data = response.json()    \n",
    "    print(response_data)\n",
    "    \n",
    "    # Handle prohibited content rejection\n",
    "    if \"candidates\" in response_data and response_data[\"candidates\"][0][\"finishReason\"] == \"PROHIBITED_CONTENT\":\n",
    "        print(\"Prohibited content detected, skipping this item.\")\n",
    "        return None\n",
    "    \n",
    "    # Check if the expected keys are present in the response\n",
    "    if \"candidates\" in response_data and len(response_data[\"candidates\"]) > 0:\n",
    "        if \"content\" in response_data[\"candidates\"][0] and \"parts\" in response_data[\"candidates\"][0][\"content\"]:\n",
    "            if len(response_data[\"candidates\"][0][\"content\"][\"parts\"]) > 0:\n",
    "                return response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "    \n",
    "    # Handle 'resource exhausted' error and other unexpected responses\n",
    "    print(\"Unexpected response structure, skipping this item.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = []\n",
    "\n",
    "for item in testing_data:\n",
    "    input_text = item['English']\n",
    "    reference_text = item['Cajun French']\n",
    "\n",
    "    # Get translation from Gemini API\n",
    "    translated_text = translate_with_gemini(input_text)\n",
    "\n",
    "    # Do not include prohibited content or other unexpected responses\n",
    "    if translated_text is None:\n",
    "        continue\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference = reference_text.split()\n",
    "    hypothesis = translated_text.split()\n",
    "    bleu_score = calculate_bleu(reference, hypothesis)\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "if bleu_scores:\n",
    "    average_bleu_score = np.mean(bleu_scores)\n",
    "    print(f\"Average BLEU score: {average_bleu_score:.4f}\")\n",
    "else:\n",
    "    print(\"No valid translations to calculate BLEU score.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
